{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d432e53",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\24897\\\\Desktop\\\\I2_ygu159\\\\Plan.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m24897\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mI2_ygu159\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPlan.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Load the Excel file\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Set the option to display all rows in the dataframe\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_rows\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/excel/_base.py:457\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    456\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    462\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/excel/_base.py:1376\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1381\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/excel/_base.py:1250\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1248\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1253\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1254\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\24897\\\\Desktop\\\\I2_ygu159\\\\Plan.xlsx'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# 01-BU\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Focus on more specific aspects of the relationship between CO2 emissions and global warming\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Project Plan\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the Excel file\n",
    "file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\Plan.xlsx'\n",
    "\n",
    "# Load the Excel file\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Set the option to display all rows in the dataframe\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Display the entire dataframe\n",
    "print(data)\n",
    "\n",
    "\n",
    "\n",
    "# 02-DU\n",
    "\n",
    "# Load Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the file\n",
    "file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\annual-co2-emissions-per-country_cleaned.xlsx'\n",
    "\n",
    "# Load the Excel file\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(data.head())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the first CSV file\n",
    "file_path1 = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Anomalies.csv'\n",
    "# Loading the first CSV file\n",
    "data1 = pd.read_csv(file_path1)\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Anomalies Data:\")\n",
    "print(data1.head())\n",
    "\n",
    "# Path to the second CSV file\n",
    "file_path2 = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\February 2024.csv'\n",
    "# Loading the second CSV file\n",
    "data2 = pd.read_csv(file_path2)\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"\\nFebruary 2024 Data:\")\n",
    "print(data2.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Explore Data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the file\n",
    "file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\annual-co2-emissions-per-country_cleaned.xlsx'\n",
    "\n",
    "# Load the Excel file\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(data.head())\n",
    "\n",
    "# Count unique entities\n",
    "num_entities = data['Entity'].nunique()\n",
    "print(f\"There are {num_entities} unique entities in the dataset.\")\n",
    "\n",
    "# Display descriptive statistics for the dataset\n",
    "print(data.describe())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path1 = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Anomalies.csv'\n",
    "\n",
    "# Loading the CSV file\n",
    "data1 = pd.read_csv(file_path1)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Anomalies Data:\")\n",
    "print(data1.head())\n",
    "\n",
    "# Print the descriptive statistics for the dataset\n",
    "print(data1.describe())\n",
    "\n",
    "# Print the data types and missing values of each column\n",
    "print(data1.info())\n",
    "\n",
    "# Check for missing values in each column\n",
    "print(data1.isnull().sum())\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path2 = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\February 2024.csv'\n",
    "\n",
    "# Loading the CSV file\n",
    "data2 = pd.read_csv(file_path2)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"\\nFebruary 2024 Data:\")\n",
    "print(data2.head())\n",
    "\n",
    "# Print the descriptive statistics for the dataset\n",
    "print(data2.describe())\n",
    "\n",
    "# Print the data types and missing values of each column\n",
    "print(data2.info())\n",
    "\n",
    "# Check for missing values in each column\n",
    "print(data2.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add Visualisations\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\annual-co2-emissions-per-country_cleaned.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Group data by year and sum emissions\n",
    "annual_emissions = data.groupby('Year')['Auunal Emission'].sum()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "annual_emissions.plot(kind='line', color='b')\n",
    "plt.title('Global CO2 Emissions Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total CO2 Emissions (in tonnes)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path1 = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Anomalies.csv'\n",
    "data1 = pd.read_csv(file_path1)\n",
    "\n",
    "# Displaying the first few rows to verify the data is loaded correctly\n",
    "print(\"Anomalies Data:\")\n",
    "print(data1.head())\n",
    "\n",
    "# Displaying descriptive statistics\n",
    "print(data1.describe())\n",
    "\n",
    "# Displaying data types and checking for missing values\n",
    "print(data1.info())\n",
    "print(data1.isnull().sum())\n",
    "\n",
    "# Plotting histograms for all numerical columns\n",
    "for column in data1.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(data1[column], bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path2 = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\February 2024.csv'\n",
    "data2 = pd.read_csv(file_path2)\n",
    "\n",
    "# Histogram to show the distribution of temperature anomalies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(data2['Anomaly'], bins=30, color='cyan', edgecolor='black')\n",
    "plt.title('Distribution of Temperature Anomalies in February 2024')\n",
    "plt.xlabel('Temperature Anomaly (°C)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Check for duplicates in the emissions dataset\n",
    "duplicate_emissions = data.duplicated(subset=['Entity', 'Year']).sum()\n",
    "print(f\"Duplicate entries: {duplicate_emissions}\")\n",
    "\n",
    "# Check for duplicates in the anomalies dataset\n",
    "duplicate_anomalies = data1.duplicated(subset=['Year', 'Month']).sum()\n",
    "print(f\"Duplicate entries: {duplicate_anomalies}\")\n",
    "\n",
    "# Check for duplicate geographic coordinates in the temperature dataset\n",
    "duplicate_temperatures = data2.duplicated(subset=['Latitude', 'Longitude']).sum()\n",
    "print(f\"Duplicate geographic entries: {duplicate_temperatures}\")\n",
    "\n",
    "\n",
    "\n",
    "# 03-DP\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the Excel file\n",
    "file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\annual-co2-emissions-per-country_cleaned.xlsx'\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Annual CO2 Emissions Data Initial Look:\")\n",
    "print(data.head())\n",
    "\n",
    "# Remove duplicate entries\n",
    "initial_count = data.shape[0]\n",
    "data.drop_duplicates(subset=['Entity', 'Year'], inplace=True)\n",
    "removed_count = initial_count - data.shape[0]\n",
    "\n",
    "print(f\"Removed {removed_count} duplicate rows.\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values_count = data.isnull().sum()\n",
    "print(\"Missing values before handling:\")\n",
    "print(missing_values_count)\n",
    "\n",
    "\n",
    "# Filling missing emissions values with the median might be more robust to outliers\n",
    "if 'Auunal Emission' in data.columns:\n",
    "    data['Auunal Emission'].fillna(data['Auunal Emission'].median(), inplace=True)\n",
    "\n",
    "# Check missing values after handling\n",
    "missing_values_count_after = data.isnull().sum()\n",
    "print(\"Missing values after handling:\")\n",
    "print(missing_values_count_after)\n",
    "\n",
    "# Check for any negative emissions values and set them to NaN or a specific value\n",
    "data['Auunal Emission'] = data['Auunal Emission'].apply(lambda x: x if x >= 0 else None)\n",
    "\n",
    "# Re-check for missing values if negative values were removed\n",
    "print(\"Updated missing values after removing negatives:\")\n",
    "print(data['Auunal Emission'].isnull().sum())\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "file_path1 = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Anomalies.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data1 = pd.read_csv(file_path1)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Anomalies Data Initial Look:\")\n",
    "print(data1.head())\n",
    "\n",
    "# Remove duplicate entries\n",
    "initial_count = data1.shape[0]\n",
    "data1.drop_duplicates(inplace=True)\n",
    "removed_count = initial_count - data1.shape[0]\n",
    "\n",
    "print(f\"Removed {removed_count} duplicate rows.\")\n",
    "# Check for missing values\n",
    "missing_values_count = data1.isnull().sum()\n",
    "print(\"Missing values before handling:\")\n",
    "print(missing_values_count)\n",
    "\n",
    "# fill missing values with the median or mean\n",
    "data1.fillna(data1.median(), inplace=True)\n",
    "\n",
    "# Check missing values after handling\n",
    "missing_values_count_after = data1.isnull().sum()\n",
    "print(\"Missing values after handling:\")\n",
    "print(missing_values_count_after)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset()\n",
    "file_path2 = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\February 2024.csv'\n",
    "data2 = pd.read_csv(file_path2)\n",
    "\n",
    "# Initially checking for and displaying the number of invalid entries\n",
    "invalid_lat_long = data2[(data2['Latitude'] < -90) | (data2['Latitude'] > 90) | \n",
    "                         (data2['Longitude'] < -180) | (data2['Longitude'] > 180)].shape[0]\n",
    "print(f\"Initial count of invalid latitude or longitude entries: {invalid_lat_long}\")\n",
    "\n",
    "# Removing entries with invalid latitude or longitude\n",
    "data2 = data2[(data2['Latitude'] >= -90) & (data2['Latitude'] <= 90) & \n",
    "              (data2['Longitude'] >= -180) & (data2['Longitude'] <= 180)]\n",
    "\n",
    "# Rechecking for invalid entries after removal\n",
    "invalid_lat_long_after = data2[(data2['Latitude'] < -90) | (data2['Latitude'] > 90) | \n",
    "                               (data2['Longitude'] < -180) | (data2['Longitude'] > 180)].shape[0]\n",
    "print(f\"Count of invalid latitude or longitude entries after cleanup: {invalid_lat_long_after}\")\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"\\nCleaned February 2024 Data:\")\n",
    "print(data2.head())\n",
    "\n",
    "# Save the cleaned Temperature dataset\n",
    "cleaned_file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Cleaned_February_2024.csv'\n",
    "data2.to_csv(cleaned_file_path, index=False) \n",
    "\n",
    "print(f\"Cleaned data saved to {cleaned_file_path}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the cleaned data file\n",
    "cleaned_file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Cleaned_February_2024.csv'\n",
    "\n",
    "# Load the cleaned dataset\n",
    "cleaned_data = pd.read_csv(cleaned_file_path)\n",
    "\n",
    "# Display the first few rows of the cleaned data to verify it's loaded correctly\n",
    "print(cleaned_data.head())\n",
    "\n",
    "# Check for basic statistics to see if the data looks consistent\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(cleaned_data.describe())\n",
    "\n",
    "#3.3 \n",
    "# Global Temp New Features\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the cleaned CSV file\n",
    "cleaned_file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Cleaned_February_2024.csv'\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_csv(cleaned_file_path)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Cleaned February 2024 Data Initial Look:\")\n",
    "print(data.head())\n",
    "\n",
    "# Calculate the mean temperature for the dataset\n",
    "mean_temperature = data['Anomaly'].mean()\n",
    "\n",
    "# Adding a 'Temp_Deviation' column that shows the deviation of each temperature from the mean\n",
    "data['Temp_Deviation'] = data['Anomaly'] - mean_temperature\n",
    "\n",
    "# Compute zonal means for temperatures grouped by latitude\n",
    "data['Zonal_Mean'] = data.groupby('Latitude')['Anomaly'].transform('mean')\n",
    "\n",
    "# Display the updated dataset with new features\n",
    "print(\"\\nData with New Features:\")\n",
    "print(data.head())\n",
    "\n",
    "# Save the modified data to a new file\n",
    "constructed_file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Constructed_February_2024_Temperatures.csv'\n",
    "data.to_csv(constructed_file_path, index=False)\n",
    "\n",
    "print(f\"Data with constructed features saved to {constructed_file_path}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the constructed CSV file\n",
    "constructed_file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Constructed_February_2024_Temperatures.csv'\n",
    "\n",
    "# Load the constructed dataset\n",
    "constructed_data = pd.read_csv(constructed_file_path)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded and check the new features\n",
    "print(\"Constructed February 2024 Data Initial Look:\")\n",
    "print(constructed_data.head())\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nSummary Statistics for New Features:\")\n",
    "print(constructed_data[['Temp_Deviation', 'Zonal_Mean']].describe())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for null values in the new features\n",
    "print(\"\\nNull Values Check in Constructed Features:\")\n",
    "print(constructed_data[['Temp_Deviation', 'Zonal_Mean']].isnull().sum())\n",
    "\n",
    "# Plotting histograms for new features\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "constructed_data['Temp_Deviation'].hist(bins=20)\n",
    "plt.title('Histogram of Temperature Deviations')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "constructed_data['Zonal_Mean'].hist(bins=20)\n",
    "plt.title('Histogram of Zonal Mean Temperatures')\n",
    "plt.show()\n",
    "\n",
    "# Anomalies New Features\n",
    "import pandas as pd\n",
    "# Specify the path to the CSV file\n",
    "file_path1 = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Anomalies.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data1 = pd.read_csv(file_path1)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Anomalies Data Initial Look:\")\n",
    "print(data1.head())\n",
    "\n",
    "# Calculate cumulative anomaly\n",
    "data1['Cumulative_Anomaly'] = data1.groupby('Year')['Value'].cumsum()\n",
    "\n",
    "# Calculate monthly averages\n",
    "data1['Monthly_Avg'] = data1.groupby('Month')['Value'].transform('mean')\n",
    "\n",
    "# Display the updated dataset with new features\n",
    "print(\"\\nData with New Features:\")\n",
    "print(data1.head())\n",
    "\n",
    "# Save the modified data to a new file\n",
    "constructed_file_path1 = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Constructed_Anomalies.csv'\n",
    "data1.to_csv(constructed_file_path1, index=False)\n",
    "\n",
    "print(f\"Data with constructed features saved to {constructed_file_path1}\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the constructed CSV file\n",
    "constructed_file_path1 = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Constructed_Anomalies.csv'\n",
    "\n",
    "# Load the constructed dataset\n",
    "constructed_data = pd.read_csv(constructed_file_path1)\n",
    "\n",
    "# Display the first few rows to verify the data and new features\n",
    "print(\"Constructed Anomalies Data Initial Look:\")\n",
    "print(constructed_data.head())\n",
    "\n",
    "# Check for continuity and correctness of 'Cumulative_Anomaly'\n",
    "print(\"\\nYear-end Cumulative Anomalies:\")\n",
    "print(constructed_data.groupby('Year')['Cumulative_Anomaly'].last())\n",
    "\n",
    "# Check that the 'Monthly_Avg' makes sense by comparing different years\n",
    "print(\"\\nSample Monthly Averages for January across Years:\")\n",
    "print(constructed_data[constructed_data['Month'] == 1]['Monthly_Avg'].unique())\n",
    "\n",
    "# Generate summary statistics for the new features\n",
    "print(\"\\nSummary Statistics for New Features:\")\n",
    "print(constructed_data[['Cumulative_Anomaly', 'Monthly_Avg']].describe())\n",
    "\n",
    "# Visualize the data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting cumulative anomalies over the years\n",
    "plt.figure(figsize=(10, 5))\n",
    "constructed_data.groupby('Year')['Cumulative_Anomaly'].last().plot(kind='line')\n",
    "plt.title('Year-end Cumulative Anomaly over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Cumulative Anomaly')\n",
    "plt.show()\n",
    "\n",
    "# Annual CO2 Emissions New features\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\annual-co2-emissions-per-country_cleaned.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Sort data to ensure proper year sequence by country\n",
    "data.sort_values(by=['Entity', 'Year'], inplace=True)\n",
    "\n",
    "# Calculate the year-on-year percentage change in emissions\n",
    "data['Emission_Change_YoY'] = data.groupby('Entity')['Auunal Emission'].pct_change() * 100\n",
    "\n",
    "# Calculate a 5-year moving average of emissions\n",
    "data['Emission_5yr_Moving_Avg'] = data.groupby('Entity')['Auunal Emission'].rolling(window=5).mean().reset_index(0,drop=True)\n",
    "\n",
    "# Display the first few rows to verify new features\n",
    "print(data[['Entity', 'Year', 'Auunal Emission', 'Emission_Change_YoY', 'Emission_5yr_Moving_Avg']].head())\n",
    "\n",
    "# Save the modified data\n",
    "constructed_file_path = r'C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Constructed_Annual_CO2_Emissions.xlsx'\n",
    "data.to_excel(constructed_file_path, index=False)\n",
    "\n",
    "# 3.4 Merge Datasets\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Anomalies dataset\n",
    "anomalies_data = pd.read_csv(r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Constructed_Anomalies.csv\")\n",
    "\n",
    "# Load the CO2 emissions dataset\n",
    "co2_data = pd.read_excel(r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Constructed_Annual_CO2_Emissions.xlsx\")\n",
    "\n",
    "# Ensure 'Year' is an integer in both datasets\n",
    "anomalies_data['Year'] = anomalies_data['Year'].astype(int)\n",
    "co2_data['Year'] = co2_data['Year'].astype(int)\n",
    "\n",
    "# Merge the datasets on 'Year'\n",
    "combined_data = pd.merge(anomalies_data, co2_data, on='Year', how='outer')\n",
    "\n",
    "# Display the first few rows to verify the merge\n",
    "print(combined_data.head())\n",
    "\n",
    "# Check for missing values and get an overview of the combined dataset\n",
    "print(\"Missing values in each column:\")\n",
    "print(combined_data.isnull().sum())\n",
    "\n",
    "# Summary statistics \n",
    "print(\"Summary statistics for combined data:\")\n",
    "print(combined_data.describe())\n",
    "\n",
    "# Save the integrated dataset\n",
    "combined_file_path = r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Integrated_Anomalies_and_CO2_Data.csv\"\n",
    "combined_data.to_csv(combined_file_path, index=False)\n",
    "\n",
    "\n",
    "#3.5 Reformat Data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the integrated dataset\n",
    "file_path = r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Integrated_Anomalies_and_CO2_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows and the dataset info\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "\n",
    "# Filter the dataset to include only records from 1950 onwards\n",
    "data = data[data['Year'] >= 1950]\n",
    "\n",
    "# Check the filtered data\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Handling missing values, for example, filling with mean, median, or removing rows\n",
    "data['Auunal Emission'].fillna(data['Auunal Emission'].median(), inplace=True)\n",
    "data['Emission_Change_YoY'].fillna(data['Emission_Change_YoY'].median(), inplace=True)\n",
    "data['Emission_5yr_Moving_Avg'].fillna(data['Emission_5yr_Moving_Avg'].median(), inplace=True)\n",
    "\n",
    "# data.dropna(subset=['Entity'], inplace=True)\n",
    "\n",
    "initial_count = data.shape[0]\n",
    "data.drop_duplicates(inplace=True)\n",
    "print(f\"Removed {initial_count - data.shape[0]} duplicates.\")\n",
    "\n",
    "# Validate data types and make any necessary adjustments\n",
    "print(data.dtypes)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "cleaned_file_path = r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Cleaned_and_Filtered_Data.csv\"\n",
    "data.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "# Generate descriptive statistics\n",
    "descriptive_stats = data.describe()\n",
    "\n",
    "# Display descriptive statistics\n",
    "print(descriptive_stats)\n",
    "\n",
    "# Calculate the total number of rows using .shape\n",
    "total_rows = data.shape[0]\n",
    "\n",
    "# Display the total number of rows\n",
    "print(\"Total number of rows in the dataset:\", total_rows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 04-DT\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Cleaned_and_Filtered_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure 'Year' and 'Entity' are in the correct format\n",
    "data['Year'] = data['Year'].astype(int)\n",
    "\n",
    "# Create a new column for the 5-year block\n",
    "data['Five_Year_Block'] = (data['Year'] // 5) * 5\n",
    "\n",
    "# Aggregate data by 'Entity' and 'Five_Year_Block'\n",
    "five_year_data = data.groupby(['Entity', 'Five_Year_Block']).mean()\n",
    "\n",
    "# Reset the index to make 'Entity' and 'Five_Year_Block' regular columns again\n",
    "five_year_data.reset_index(inplace=True)\n",
    "\n",
    "# Display the aggregated data to verify correctness\n",
    "print(five_year_data.head())\n",
    "\n",
    "# Save the aggregated data\n",
    "aggregated_file_path = r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Entity_Specific_Five_Year_Aggregated_Data.csv\"\n",
    "five_year_data.to_csv(aggregated_file_path, index=False)\n",
    "\n",
    "\n",
    "#4.2 Project Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Entity_Specific_Five_Year_Aggregated_Data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to ensure correct loading\n",
    "print(data.head())\n",
    "\n",
    "# Check skewness for an example column 'Auunal Emission'\n",
    "print(\"Skewness before transformation:\", data['Auunal Emission'].skew())\n",
    "\n",
    "# Apply a logarithmic transformation, adding 1 to avoid log(0)\n",
    "data['Auunal Emission'] = np.log(data['Auunal Emission'] + 1)\n",
    "\n",
    "# Check skewness after transformation\n",
    "print(\"Skewness after transformation:\", data['Auunal Emission'].skew())\n",
    "\n",
    "# Plotting the distribution before and after log transformation\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "data['Auunal Emission'].hist(bins=30)\n",
    "plt.title('Log Transformed Distribution of Emissions')\n",
    "plt.xlabel('Log of Auunal Emission')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the transformed dataset\n",
    "transformed_file_path = r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Transformed_Five_Year_Aggregated_Data.csv\"\n",
    "data.to_csv(transformed_file_path, index=False)\n",
    "\n",
    "print(\"Transformed data saved successfully.\")\n",
    "\n",
    "\n",
    "# 05-DMM\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Identify the Data Mining method\n",
    "Describe how it aligns with the objectives\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 06-DMA\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Load time series data\n",
    "data = pd.read_csv(r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Transformed_Five_Year_Aggregated_Data.csv\")\n",
    "\n",
    "# Assuming 'Year' is the index and 'Auunal Emission' is the time series column you want to model\n",
    "data['Year'] = pd.to_datetime(data['Year'], format='%Y')  # Ensure 'Year' is a datetime type\n",
    "data.set_index('Year', inplace=True)\n",
    "\n",
    "# Example parameter settings after exploratory analysis\n",
    "model = ARIMA(data['Auunal Emission'], order=(1, 1, 1))  # These parameters should be refined based on your data\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Print summary of the model to review performance and diagnostics\n",
    "print(fitted_model.summary())\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Transformed_Five_Year_Aggregated_Data.csv\")\n",
    "\n",
    "# Assume that 'Emission' and another column like 'Cumulative_Anomaly' are used for clustering\n",
    "features = data[['Auunal Emission', 'Cumulative_Anomaly']]\n",
    "\n",
    "# Use the Elbow method to determine the optimal number of clusters\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(features)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')  # Within-cluster sum of squares\n",
    "plt.show()\n",
    "\n",
    "# Fitting K-Means to the dataset\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)  # Adjust the number of clusters based on the elbow curve\n",
    "kmeans.fit(features)\n",
    "data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# check the resulting clusters\n",
    "print(data[['Auunal Emission', 'Cumulative_Anomaly', 'Cluster']].head())\n",
    "\n",
    "\n",
    "\n",
    "# 07-DM\n",
    "\n",
    "# 7.2\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Load the time series data\n",
    "data = pd.read_csv(r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Transformed_Five_Year_Aggregated_Data.csv\")\n",
    "data['Year'] = pd.to_datetime(data['Year'], format='%Y')\n",
    "data.set_index('Year', inplace=True)\n",
    "data.index = pd.DatetimeIndex(data.index).to_period('Y')  # Ensure yearly frequency\n",
    "\n",
    "# Fit the ARIMA model\n",
    "model = ARIMA(data['Auunal Emission'], order=(1, 1, 1))\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Forecast future points and handle index correctly\n",
    "forecast = fitted_model.forecast(steps=5)\n",
    "print(\"Forecasted Emissions:\", forecast)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Transformed_Five_Year_Aggregated_Data.csv\")\n",
    "\n",
    "# Select features for clustering\n",
    "features = data[['Auunal Emission', 'Cumulative_Anomaly']]\n",
    "\n",
    "# Create and fit the K-Means model\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "data['Cluster'] = kmeans.fit_predict(features)\n",
    "\n",
    "# Examine cluster centers and assign meaningful labels or analysis\n",
    "print(\"Cluster Centers:\", kmeans.cluster_centers_)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 08-INT\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Transformed_Five_Year_Aggregated_Data.csv\")\n",
    "data['Year'] = pd.to_datetime(data['Year'], format='%Y')\n",
    "data.set_index('Year', inplace=True)\n",
    "data = data.sort_index()  # 确保数据是按照日期排序的\n",
    "\n",
    "# Fit the ARIMA model\n",
    "model = ARIMA(data['Auunal Emission'], order=(1, 1, 1))\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Forecast future points\n",
    "forecast = fitted_model.forecast(steps=5)\n",
    "forecast_index = pd.date_range(start=data.index.max(), periods=len(forecast) + 1, freq='A')[1:]\n",
    "\n",
    "# Plot historical data and forecasts\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data.index, data['Auunal Emission'], label='Historical Emissions')\n",
    "plt.plot(forecast_index, forecast, label='Forecasted Emissions', color='red')\n",
    "plt.title('ARIMA Model Forecast')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Annual Emissions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "residuals = fitted_model.resid\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(residuals)\n",
    "plt.title('Residuals from ARIMA Model')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Kmeans\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\24897\\Desktop\\I2_ygu159\\I2_ygu159\\Transformed_Five_Year_Aggregated_Data.csv\")\n",
    "\n",
    "# Select features for clustering\n",
    "features = data[['Auunal Emission', 'Cumulative_Anomaly']]\n",
    "\n",
    "# Create and fit the K-Means model\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "data['Cluster'] = kmeans.fit_predict(features)\n",
    "\n",
    "# Plot the clustering scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['Auunal Emission'], data['Cumulative_Anomaly'], c=data['Cluster'], cmap='viridis', marker='o')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.xlabel('Annual Emission')\n",
    "plt.ylabel('Cumulative Anomaly')\n",
    "plt.title('K-Means Clustering of Emission Data')\n",
    "plt.show()\n",
    "\n",
    "# Plot cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='x')\n",
    "plt.xlabel('Annual Emission')\n",
    "plt.ylabel('Cumulative Anomaly')\n",
    "plt.title('Centers of K-Means Clusters')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 09-ACT\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Desribe the Action Plan to Implement, Observe and Improve\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a9852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
