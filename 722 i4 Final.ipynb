{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4d6e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------------+-------------------+---------------+\n",
      "|Step|                Task|         Start Date|           End Date|Duration (Days)|\n",
      "+----+--------------------+-------------------+-------------------+---------------+\n",
      "| 1.0|Business/Situatio...|2024-04-01 00:00:00|2024-04-03 00:00:00|            3.0|\n",
      "| 1.1| Identify Objectives|2024-04-01 00:00:00|2024-04-01 00:00:00|            1.0|\n",
      "| 1.2|    Assess Situation|2024-04-02 00:00:00|2024-04-02 00:00:00|            1.0|\n",
      "| 1.3|Determine Data Mi...|2024-04-03 00:00:00|2024-04-03 00:00:00|            1.0|\n",
      "| 1.4|Produce Project Plan|2024-04-03 00:00:00|2024-04-03 00:00:00|            1.0|\n",
      "| 2.0|  Data Understanding|2024-04-04 00:00:00|2024-04-06 00:00:00|            3.0|\n",
      "| 2.1|Collect Initial Data|2024-04-04 00:00:00|2024-04-04 00:00:00|            1.0|\n",
      "| 2.2|   Describe the Data|2024-04-05 00:00:00|2024-04-05 00:00:00|            1.0|\n",
      "| 2.3|    Explore the Data|2024-04-06 00:00:00|2024-04-06 00:00:00|            1.0|\n",
      "| 2.4| Verify Data Quality|2024-04-06 00:00:00|2024-04-06 00:00:00|            1.0|\n",
      "| 3.0|    Data Preparation|2024-04-07 00:00:00|2024-04-09 00:00:00|            3.0|\n",
      "| 3.1|     Select the Data|2024-04-07 00:00:00|2024-04-07 00:00:00|            1.0|\n",
      "| 3.2|      Clean the Data|2024-04-08 00:00:00|2024-04-08 00:00:00|            1.0|\n",
      "| 3.3|  Construct the Data|2024-04-09 00:00:00|2024-04-09 00:00:00|            1.0|\n",
      "| 3.4|Integrate Various...|2024-04-09 00:00:00|2024-04-09 00:00:00|            1.0|\n",
      "| 3.5|Format the Data a...|2024-04-09 00:00:00|2024-04-09 00:00:00|            1.0|\n",
      "| 4.0| Data Transformation|2024-04-10 00:00:00|2024-04-11 00:00:00|            2.0|\n",
      "| 4.1|     Reduce the Data|2024-04-10 00:00:00|2024-04-10 00:00:00|            1.0|\n",
      "| 4.2|    Project the Data|2024-04-11 00:00:00|2024-04-11 00:00:00|            1.0|\n",
      "| 5.0|Data-Mining Metho...|2024-04-12 00:00:00|2024-04-12 00:00:00|            1.0|\n",
      "+----+--------------------+-------------------+-------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+--------------------+----------+----------+---------------+\n",
      "|Step|                Task|Start Date|  End Date|Duration (Days)|\n",
      "+----+--------------------+----------+----------+---------------+\n",
      "| 1.0|Business/Situatio...|2024-05-01|2024-05-03|            3.0|\n",
      "| 1.1| Identify Objectives|2024-05-01|2024-05-01|            1.0|\n",
      "| 1.2|    Assess Situation|2024-05-02|2024-05-02|            1.0|\n",
      "| 1.3|Determine Data Mi...|2024-05-03|2024-05-03|            1.0|\n",
      "| 1.4|Produce Project Plan|2024-05-03|2024-05-03|            1.0|\n",
      "| 2.0|  Data Understanding|2024-05-04|2024-05-06|            3.0|\n",
      "| 2.1|Collect Initial Data|2024-05-04|2024-05-04|            1.0|\n",
      "| 2.2|   Describe the Data|2024-05-05|2024-05-05|            1.0|\n",
      "| 2.3|    Explore the Data|2024-05-06|2024-05-06|            1.0|\n",
      "| 2.4| Verify Data Quality|2024-05-06|2024-05-06|            1.0|\n",
      "| 3.0|    Data Preparation|2024-05-07|2024-05-09|            3.0|\n",
      "| 3.1|     Select the Data|2024-05-07|2024-05-07|            1.0|\n",
      "| 3.2|      Clean the Data|2024-05-08|2024-05-08|            1.0|\n",
      "| 3.3|  Construct the Data|2024-05-09|2024-05-09|            1.0|\n",
      "| 3.4|Integrate Various...|2024-05-09|2024-05-09|            1.0|\n",
      "| 3.5|Format the Data a...|2024-05-09|2024-05-09|            1.0|\n",
      "| 4.0| Data Transformation|2024-05-10|2024-05-11|            2.0|\n",
      "| 4.1|     Reduce the Data|2024-05-10|2024-05-10|            1.0|\n",
      "| 4.2|    Project the Data|2024-05-11|2024-05-11|            1.0|\n",
      "| 5.0|Data-Mining Metho...|2024-05-12|2024-05-12|            1.0|\n",
      "+----+--------------------+----------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "(36, 5)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "# Path to the Excel file\n",
    "file = 'Plan.xlsx'\n",
    "\n",
    "# Create a Spark session with the correct package\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExcelFileReader\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the Excel file\n",
    "try:\n",
    "    df = spark.read \\\n",
    "        .format(\"com.crealytics.spark.excel\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(file)\n",
    "    \n",
    "    # Show the first few rows of the DataFrame\n",
    "    df.show()\n",
    "\n",
    "    # Update the 'Start Date' and 'End Date' columns to replace April with May\n",
    "    df = df.withColumn('Start Date', regexp_replace(col('Start Date').cast('string'), '-04-', '-05-'))\n",
    "    df = df.withColumn('End Date', regexp_replace(col('End Date').cast('string'), '-04-', '-05-'))\n",
    "\n",
    "    # Convert the updated string columns back to date type if needed\n",
    "    df = df.withColumn('Start Date', col('Start Date').cast('date'))\n",
    "    df = df.withColumn('End Date', col('End Date').cast('date'))\n",
    "\n",
    "    # Show the modified DataFrame\n",
    "    df.show()\n",
    "\n",
    "    # Print the shape of the DataFrame (number of rows and columns)\n",
    "    print((df.count(), len(df.columns)))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the Excel file: {e}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf0ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExcelAndCSVReader\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the Excel file\n",
    "excel_data = spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load('annual-co2-emissions-per-country_cleaned.xlsx')\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Excel Data:\")\n",
    "excel_data.show(5)\n",
    "\n",
    "# Load the first CSV file\n",
    "csv_data1 = spark.read.csv('Anomalies.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Anomalies Data:\")\n",
    "csv_data1.show(5)\n",
    "\n",
    "# Load the second CSV file\n",
    "csv_data2 = spark.read.csv('February 2024.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"\\nFebruary 2024 Data:\")\n",
    "csv_data2.show(5)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExcelDataExplorer\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Specify the path to the Excel file\n",
    "file_path = 'annual-co2-emissions-per-country_cleaned.xlsx'\n",
    "\n",
    "# Read the Excel file\n",
    "data = spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"First few rows of the dataset:\")\n",
    "data.show(5)\n",
    "\n",
    "# Count unique entities\n",
    "num_entities = data.select('Entity').distinct().count()\n",
    "print(f\"There are {num_entities} unique entities in the dataset.\")\n",
    "\n",
    "# Display descriptive statistics for the dataset\n",
    "print(\"Descriptive statistics for the dataset:\")\n",
    "data.describe().show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSVDataExplorer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the first CSV file\n",
    "file_path1 = 'Anomalies.csv'\n",
    "data1 = spark.read.csv(file_path1, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Anomalies Data:\")\n",
    "data1.show(5)\n",
    "\n",
    "# Print the descriptive statistics for the dataset\n",
    "print(\"Descriptive statistics for Anomalies dataset:\")\n",
    "data1.describe().show()\n",
    "\n",
    "# Print the data types and missing values of each column\n",
    "print(\"Data types and missing values for Anomalies dataset:\")\n",
    "data1.printSchema()\n",
    "data1.select([col(c).isNull().alias(c) for c in data1.columns]).groupBy().sum().show()\n",
    "\n",
    "# Load the second CSV file\n",
    "file_path2 = 'February 2024.csv'\n",
    "data2 = spark.read.csv(file_path2, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"\\nFebruary 2024 Data:\")\n",
    "data2.show(5)\n",
    "\n",
    "# Print the descriptive statistics for the dataset\n",
    "print(\"Descriptive statistics for February 2024 dataset:\")\n",
    "data2.describe().show()\n",
    "\n",
    "# Print the data types and missing values of each column\n",
    "print(\"Data types and missing values for February 2024 dataset:\")\n",
    "data2.printSchema()\n",
    "data2.select([col(c).isNull().alias(c) for c in data2.columns]).groupBy().sum().show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataVisualizations\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'annual-co2-emissions-per-country_cleaned.xlsx'\n",
    "data = spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "# Group data by year and sum emissions\n",
    "annual_emissions = data.groupBy('Year').agg(_sum('Auunal Emission').alias('Total Emission'))\n",
    "\n",
    "# Convert to pandas DataFrame for plotting\n",
    "annual_emissions_pd = annual_emissions.toPandas().sort_values('Year')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(annual_emissions_pd['Year'], annual_emissions_pd['Total Emission'], color='b')\n",
    "plt.title('Global CO2 Emissions Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total CO2 Emissions (in tonnes)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AnomaliesDataVisualization\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file\n",
    "file_path1 = 'Anomalies.csv'\n",
    "data1 = spark.read.csv(file_path1, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows to verify the data is loaded\n",
    "print(\"Anomalies Data:\")\n",
    "data1.show(5)\n",
    "\n",
    "# Print the descriptive statistics for the dataset\n",
    "print(\"Descriptive statistics for Anomalies dataset:\")\n",
    "data1.describe().show()\n",
    "\n",
    "# Print the data types and missing values of each column\n",
    "print(\"Data types for Anomalies dataset:\")\n",
    "data1.printSchema()\n",
    "\n",
    "# Check for missing values in each column\n",
    "missing_values = data1.select([col(c).isNull().alias(c) for c in data1.columns]).groupBy().sum()\n",
    "missing_values.show()\n",
    "\n",
    "# Convert to pandas DataFrame for plotting\n",
    "data1_pd = data1.toPandas()\n",
    "\n",
    "# Plotting histograms for all numerical columns\n",
    "numerical_columns = data1_pd.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "for column in numerical_columns:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(data1_pd[column], bins=30, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataVisualizations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the CSV file\n",
    "file_path2 = 'February 2024.csv'\n",
    "data2 = spark.read.csv(file_path2, header=True, inferSchema=True)\n",
    "\n",
    "# Show the first few rows to verify the data is loaded\n",
    "print(\"\\nFebruary 2024 Data:\")\n",
    "data2.show(5)\n",
    "\n",
    "# Plotting histogram to show the distribution of temperature anomalies\n",
    "data2_pd = data2.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(data2_pd['Anomaly'], bins=30, color='cyan', edgecolor='black')\n",
    "plt.title('Distribution of Temperature Anomalies in February 2024')\n",
    "plt.xlabel('Temperature Anomaly (°C)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataVisualizations\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the datasets\n",
    "file_path = 'annual-co2-emissions-per-country_cleaned.xlsx'\n",
    "data = spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "file_path1 = 'Anomalies.csv'\n",
    "data1 = spark.read.csv(file_path1, header=True, inferSchema=True)\n",
    "\n",
    "file_path2 = 'February 2024.csv'\n",
    "data2 = spark.read.csv(file_path2, header=True, inferSchema=True)\n",
    "\n",
    "# Check for duplicates in the emissions dataset\n",
    "duplicate_emissions = data.groupBy('Entity', 'Year').count().filter(col('count') > 1).count()\n",
    "print(f\"Duplicate entries in emissions dataset: {duplicate_emissions}\")\n",
    "\n",
    "# Check for duplicates in the anomalies dataset\n",
    "duplicate_anomalies = data1.groupBy('Year', 'Month').count().filter(col('count') > 1).count()\n",
    "print(f\"Duplicate entries in anomalies dataset: {duplicate_anomalies}\")\n",
    "\n",
    "# Check for duplicate geographic coordinates in the temperature dataset\n",
    "duplicate_temperatures = data2.groupBy('Latitude', 'Longitude').count().filter(col('count') > 1).count()\n",
    "print(f\"Duplicate geographic entries in temperature dataset: {duplicate_temperatures}\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8727c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnull, when, count\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataProcessing\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Function to save DataFrame to CSV\n",
    "def save_dataframe_to_csv(df, file_path):\n",
    "    df.coalesce(1).write.csv(file_path, header=True, mode='overwrite')\n",
    "\n",
    "### Step 2: Load and Clean the Excel Dataset\n",
    "\n",
    "# Specify the path to the Excel file\n",
    "file_path = 'annual-co2-emissions-per-country_cleaned.xlsx'\n",
    "\n",
    "# Load the dataset\n",
    "data = spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Annual CO2 Emissions Data Initial Look:\")\n",
    "data.show(5)\n",
    "\n",
    "# Remove duplicate entries\n",
    "data = data.dropDuplicates(['Entity', 'Year'])\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values before handling:\")\n",
    "data.select([count(when(isnull(c), c)).alias(c) for c in data.columns]).show()\n",
    "\n",
    "# Fill missing emissions values with the median\n",
    "if 'Auunal Emission' in data.columns:\n",
    "    median_value = data.approxQuantile('Auunal Emission', [0.5], 0.25)[0]\n",
    "    data = data.withColumn('Auunal Emission', when(isnull(col('Auunal Emission')), median_value).otherwise(col('Auunal Emission')))\n",
    "\n",
    "# Check missing values after handling\n",
    "print(\"Missing values after handling:\")\n",
    "data.select([count(when(isnull(c), c)).alias(c) for c in data.columns]).show()\n",
    "\n",
    "# Check for any negative emissions values and set them to NaN\n",
    "data = data.withColumn('Auunal Emission', when(col('Auunal Emission') >= 0, col('Auunal Emission')).otherwise(None))\n",
    "\n",
    "# Re-check for missing values if negative values were removed\n",
    "print(\"Updated missing values after removing negatives:\")\n",
    "data.select([count(when(isnull(c), c)).alias(c) for c in data.columns]).show()\n",
    "\n",
    "# Save the cleaned dataset\n",
    "save_dataframe_to_csv(data, 'Cleaned_annual_co2_emissions.csv')\n",
    "\n",
    "### Step 3: Load and Clean the First CSV Dataset\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "file_path1 = 'Anomalies.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data1 = spark.read.csv(file_path1, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Anomalies Data Initial Look:\")\n",
    "data1.show(5)\n",
    "\n",
    "# Remove duplicate entries\n",
    "data1 = data1.dropDuplicates()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values before handling:\")\n",
    "data1.select([count(when(isnull(c), c)).alias(c) for c in data1.columns]).show()\n",
    "\n",
    "# Fill missing values with the median\n",
    "for col_name in data1.columns:\n",
    "    median_value = data1.approxQuantile(col_name, [0.5], 0.25)[0]\n",
    "    data1 = data1.withColumn(col_name, when(isnull(col(col_name)), median_value).otherwise(col(col_name)))\n",
    "\n",
    "# Check missing values after handling\n",
    "print(\"Missing values after handling:\")\n",
    "data1.select([count(when(isnull(c), c)).alias(c) for c in data1.columns]).show()\n",
    "\n",
    "# Save the cleaned dataset\n",
    "save_dataframe_to_csv(data1, 'Cleaned_Anomalies.csv')\n",
    "\n",
    "### Step 4: Load and Clean the Second CSV Dataset\n",
    "\n",
    "# Specify the path to the CSV file\n",
    "file_path2 = 'February 2024.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data2 = spark.read.csv(file_path2, header=True, inferSchema=True)\n",
    "\n",
    "# Initially checking for and displaying the number of invalid entries\n",
    "invalid_lat_long = data2.filter((col('Latitude') < -90) | (col('Latitude') > 90) | \n",
    "                                (col('Longitude') < -180) | (col('Longitude') > 180)).count()\n",
    "print(f\"Initial count of invalid latitude or longitude entries: {invalid_lat_long}\")\n",
    "\n",
    "# Removing entries with invalid latitude or longitude\n",
    "data2 = data2.filter((col('Latitude') >= -90) & (col('Latitude') <= 90) & \n",
    "                     (col('Longitude') >= -180) & (col('Longitude') <= 180))\n",
    "\n",
    "# Rechecking for invalid entries after removal\n",
    "invalid_lat_long_after = data2.filter((col('Latitude') < -90) | (col('Latitude') > 90) | \n",
    "                                      (col('Longitude') < -180) | (col('Longitude') > 180)).count()\n",
    "print(f\"Count of invalid latitude or longitude entries after cleanup: {invalid_lat_long_after}\")\n",
    "\n",
    "# Display the cleaned data\n",
    "print(\"\\nCleaned February 2024 Data:\")\n",
    "data2.show(5)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "save_dataframe_to_csv(data2, 'Cleaned_February_2024.csv')\n",
    "\n",
    "### Step 5: Verify the Cleaned Data\n",
    "\n",
    "# Load the cleaned dataset\n",
    "cleaned_file_path = 'Cleaned_February_2024.csv'\n",
    "cleaned_data = spark.read.csv(cleaned_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows of the cleaned data to verify it's loaded correctly\n",
    "cleaned_data.show(5)\n",
    "\n",
    "# Check for basic statistics to see if the data looks consistent\n",
    "print(\"\\nBasic Statistics:\")\n",
    "cleaned_data.describe().show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, mean as _mean, lag, avg\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NewFeatures\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .getOrCreate()\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Load the cleaned dataset\n",
    "file_path = 'Cleaned_February_2024.csv'\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Cleaned February 2024 Data Initial Look:\")\n",
    "data.show(5)\n",
    "\n",
    "# Calculate the mean temperature for the dataset\n",
    "mean_temperature = data.select(_mean(col('Anomaly'))).collect()[0][0]\n",
    "\n",
    "# Adding a 'Temp_Deviation' column that shows the deviation of each temperature from the mean\n",
    "data = data.withColumn('Temp_Deviation', col('Anomaly') - mean_temperature)\n",
    "\n",
    "# Compute zonal means for temperatures grouped by latitude\n",
    "window_spec = Window.partitionBy('Latitude')\n",
    "data = data.withColumn('Zonal_Mean', avg(col('Anomaly')).over(window_spec))\n",
    "\n",
    "# Display the updated dataset with new features\n",
    "print(\"\\nData with New Features:\")\n",
    "data.show(5)\n",
    "\n",
    "# Save the modified data to a new file\n",
    "constructed_file_path = 'Constructed_February_2024_Temperatures.csv'\n",
    "data.coalesce(1).write.csv(constructed_file_path, header=True, mode='overwrite')\n",
    "\n",
    "print(f\"Data with constructed features saved to {constructed_file_path}\")\n",
    "\n",
    "# Verify the saved data\n",
    "constructed_data = spark.read.csv(constructed_file_path, header=True, inferSchema=True)\n",
    "print(\"Constructed February 2024 Data Initial Look:\")\n",
    "constructed_data.show(5)\n",
    "\n",
    "# Summary statistics for new features\n",
    "print(\"\\nSummary Statistics for New Features:\")\n",
    "constructed_data.select('Temp_Deviation', 'Zonal_Mean').describe().show()\n",
    "\n",
    "# Check for null values in the new features\n",
    "print(\"\\nNull Values Check in Constructed Features:\")\n",
    "constructed_data.select([count(when(isnull(c), c)).alias(c) for c in ['Temp_Deviation', 'Zonal_Mean']]).show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting histograms for new features\n",
    "constructed_data_pd = constructed_data.toPandas()\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "constructed_data_pd['Temp_Deviation'].hist(bins=20)\n",
    "plt.title('Histogram of Temperature Deviations')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "constructed_data_pd['Zonal_Mean'].hist(bins=20)\n",
    "plt.title('Histogram of Zonal Mean Temperatures')\n",
    "plt.show()\n",
    "# Load the dataset\n",
    "file_path1 = 'Anomalies.csv'\n",
    "data1 = spark.read.csv(file_path1, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows to verify the data is loaded\n",
    "print(\"Anomalies Data Initial Look:\")\n",
    "data1.show(5)\n",
    "\n",
    "# Calculate cumulative anomaly\n",
    "window_spec = Window.partitionBy('Year').orderBy('Month')\n",
    "data1 = data1.withColumn('Cumulative_Anomaly', expr('sum(Value) over (partition by Year order by Month)'))\n",
    "\n",
    "# Calculate monthly averages\n",
    "window_spec = Window.partitionBy('Month')\n",
    "data1 = data1.withColumn('Monthly_Avg', avg(col('Value')).over(window_spec))\n",
    "\n",
    "# Display the updated dataset with new features\n",
    "print(\"\\nData with New Features:\")\n",
    "data1.show(5)\n",
    "\n",
    "# Save the modified data to a new file\n",
    "constructed_file_path1 = 'Constructed_Anomalies.csv'\n",
    "data1.coalesce(1).write.csv(constructed_file_path1, header=True, mode='overwrite')\n",
    "\n",
    "print(f\"Data with constructed features saved to {constructed_file_path1}\")\n",
    "\n",
    "# Verify the saved data\n",
    "constructed_data = spark.read.csv(constructed_file_path1, header=True, inferSchema=True)\n",
    "print(\"Constructed Anomalies Data Initial Look:\")\n",
    "constructed_data.show(5)\n",
    "\n",
    "# Check for continuity and correctness of 'Cumulative_Anomaly'\n",
    "print(\"\\nYear-end Cumulative Anomalies:\")\n",
    "constructed_data.groupBy('Year').agg(expr('max(Cumulative_Anomaly)').alias('Year-end Cumulative Anomaly')).show()\n",
    "\n",
    "# Check that the 'Monthly_Avg' makes sense by comparing different years\n",
    "print(\"\\nSample Monthly Averages for January across Years:\")\n",
    "constructed_data.filter(col('Month') == 1).select('Monthly_Avg').distinct().show()\n",
    "\n",
    "# Summary statistics for new features\n",
    "print(\"\\nSummary Statistics for New Features:\")\n",
    "constructed_data.select('Cumulative_Anomaly', 'Monthly_Avg').describe().show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting cumulative anomalies over the years\n",
    "constructed_data_pd = constructed_data.toPandas()\n",
    "plt.figure(figsize=(10, 5))\n",
    "constructed_data_pd.groupby('Year')['Cumulative_Anomaly'].last().plot(kind='line')\n",
    "plt.title('Year-end Cumulative Anomaly over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Cumulative Anomaly')\n",
    "plt.show()\n",
    "# Load the dataset\n",
    "file_path = 'annual-co2-emissions-per-country_cleaned.xlsx'\n",
    "data = spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "# Sort data to ensure proper year sequence by country\n",
    "data = data.orderBy('Entity', 'Year')\n",
    "\n",
    "# Calculate the year-on-year percentage change in emissions\n",
    "window_spec = Window.partitionBy('Entity').orderBy('Year')\n",
    "data = data.withColumn('Emission_Change_YoY', (col('Auunal Emission') - lag(col('Auunal Emission')).over(window_spec)) / lag(col('Auunal Emission')).over(window_spec) * 100)\n",
    "\n",
    "# Calculate a 5-year moving average of emissions\n",
    "data = data.withColumn('Emission_5yr_Moving_Avg', avg(col('Auunal Emission')).over(window_spec.rowsBetween(-4, 0)))\n",
    "\n",
    "# Display the first few rows to verify new features\n",
    "print(data.select('Entity', 'Year', 'Auunal Emission', 'Emission_Change_YoY', 'Emission_5yr_Moving_Avg').show(5))\n",
    "\n",
    "# Save the modified data\n",
    "constructed_file_path = 'Constructed_Annual_CO2_Emissions.xlsx'\n",
    "data.coalesce(1).write.format('com.databricks.spark.csv').option('header', 'true').save(constructed_file_path, mode='overwrite')\n",
    "\n",
    "print(f\"Data with constructed features saved to {constructed_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76711890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, isnull\n",
    "import os\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MergeDatasets\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = 'Constructed_Annual_CO2_Emissions.xlsx'\n",
    "\n",
    "# Verify if the path is a directory and list contents\n",
    "if os.path.isdir(directory_path):\n",
    "    print(f\"The path {directory_path} is a directory. Listing contents:\")\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        print(file_name)\n",
    "else:\n",
    "    print(f\"The path {directory_path} is not a directory. Please check the path.\")\n",
    "\n",
    "# Assuming the correct file is found in the directory, update the file path\n",
    "file_path = os.path.join(directory_path, 'part-00000-1592ac19-4f26-49a5-9a2e-ef670bb1be21-c000.csv')  # Replace with actual file name if different\n",
    "\n",
    "# Check if the updated file path exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"File not found: {file_path}\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the CO2 emissions dataset\n",
    "        co2_data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Verify the CO2 emissions dataset\n",
    "        print(\"CO2 Emissions Data Initial Look:\")\n",
    "        co2_data.show(5)\n",
    "\n",
    "        # Proceed only if co2_data is loaded successfully\n",
    "        # Load the Anomalies dataset\n",
    "        anomalies_data_path = 'Constructed_Anomalies.csv'\n",
    "        anomalies_data = spark.read.csv(anomalies_data_path, header=True, inferSchema=True)\n",
    "\n",
    "        # Verify the Anomalies dataset\n",
    "        print(\"Anomalies Data Initial Look:\")\n",
    "        anomalies_data.show(5)\n",
    "\n",
    "        # Ensure 'Year' is an integer in both datasets\n",
    "        anomalies_data = anomalies_data.withColumn('Year', col('Year').cast('integer'))\n",
    "        co2_data = co2_data.withColumn('Year', col('Year').cast('integer'))\n",
    "\n",
    "        # Merge the datasets on 'Year'\n",
    "        combined_data = anomalies_data.join(co2_data, on='Year', how='outer')\n",
    "\n",
    "        # Display the first few rows to verify the merge\n",
    "        print(\"Combined Data Initial Look:\")\n",
    "        combined_data.show(5)\n",
    "\n",
    "        # Check for missing values in each column\n",
    "        print(\"Missing values in each column:\")\n",
    "        combined_data.select([count(when(isnull(c), c)).alias(c) for c in combined_data.columns]).show()\n",
    "\n",
    "        # Summary statistics for the combined data\n",
    "        print(\"Summary statistics for combined data:\")\n",
    "        combined_data.describe().show()\n",
    "\n",
    "        # Save the integrated dataset\n",
    "        combined_file_path = 'Integrated_Anomalies_and_CO2_Data.csv'\n",
    "        combined_data.coalesce(1).write.csv(combined_file_path, header=True, mode='overwrite')\n",
    "\n",
    "        print(f\"Integrated data saved to {combined_file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the CSV file: {e}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count, mean\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReformatData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the integrated dataset\n",
    "file_path = 'Integrated_Anomalies_and_CO2_Data.csv'\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows and the dataset info\n",
    "print(\"Initial Data:\")\n",
    "data.show(5)\n",
    "data.printSchema()\n",
    "\n",
    "# Filter the dataset to include only records from 1950 onwards\n",
    "data = data.filter(col('Year') >= 1950)\n",
    "\n",
    "# Check the filtered data\n",
    "print(\"Filtered Data (from 1950 onwards):\")\n",
    "data.show(5)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()\n",
    "\n",
    "# Handling missing values by filling with median values\n",
    "median_auunal_emission = data.approxQuantile('Auunal Emission', [0.5], 0.25)[0]\n",
    "median_emission_change_yoy = data.approxQuantile('Emission_Change_YoY', [0.5], 0.25)[0]\n",
    "median_emission_5yr_moving_avg = data.approxQuantile('Emission_5yr_Moving_Avg', [0.5], 0.25)[0]\n",
    "\n",
    "data = data.fillna({\n",
    "    'Auunal Emission': median_auunal_emission,\n",
    "    'Emission_Change_YoY': median_emission_change_yoy,\n",
    "    'Emission_5yr_Moving_Avg': median_emission_5yr_moving_avg\n",
    "})\n",
    "\n",
    "# Remove duplicates\n",
    "initial_count = data.count()\n",
    "data = data.dropDuplicates()\n",
    "print(f\"Removed {initial_count - data.count()} duplicates.\")\n",
    "\n",
    "# Validate data types and make any necessary adjustments\n",
    "print(\"Data Types:\")\n",
    "data.printSchema()\n",
    "\n",
    "# Save the cleaned dataset\n",
    "cleaned_file_path = 'Cleaned_and_Filtered_Data.csv'\n",
    "data.coalesce(1).write.csv(cleaned_file_path, header=True, mode='overwrite')\n",
    "\n",
    "# Generate descriptive statistics\n",
    "descriptive_stats = data.describe()\n",
    "descriptive_stats.show()\n",
    "\n",
    "# Calculate the total number of rows using .count()\n",
    "total_rows = data.count()\n",
    "\n",
    "# Display the total number of rows\n",
    "print(\"Total number of rows in the dataset:\", total_rows)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea781a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.1\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, floor, avg\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AggregateData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Cleaned_and_Filtered_Data.csv'\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Ensure 'Year' is in the correct format\n",
    "data = data.withColumn('Year', col('Year').cast('int'))\n",
    "\n",
    "# Create a new column for the 5-year block\n",
    "data = data.withColumn('Five_Year_Block', floor(col('Year') / 5) * 5)\n",
    "\n",
    "# Aggregate data by 'Entity' and 'Five_Year_Block'\n",
    "five_year_data = data.groupBy('Entity', 'Five_Year_Block').agg(\n",
    "    *[avg(c).alias(c) for c in data.columns if c not in ['Entity', 'Five_Year_Block', 'Year']]\n",
    ")\n",
    "\n",
    "# Display the aggregated data to verify correctness\n",
    "print(\"Aggregated Data:\")\n",
    "five_year_data.show(5)\n",
    "\n",
    "# Save the aggregated data\n",
    "aggregated_file_path = 'Entity_Specific_Five_Year_Aggregated_Data.csv'\n",
    "five_year_data.coalesce(1).write.csv(aggregated_file_path, header=True, mode='overwrite')\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, log1p, skewness\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProjectData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Entity_Specific_Five_Year_Aggregated_Data.csv'\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows to ensure correct loading\n",
    "print(\"Initial Data:\")\n",
    "data.show(5)\n",
    "\n",
    "# Check skewness for an example column 'Auunal Emission' before transformation\n",
    "initial_skewness = data.select(skewness(col('Auunal Emission'))).collect()[0][0]\n",
    "print(\"Skewness before transformation:\", initial_skewness)\n",
    "\n",
    "# Apply a logarithmic transformation, adding 1 to avoid log(0)\n",
    "data = data.withColumn('Auunal Emission', log1p(col('Auunal Emission')))\n",
    "\n",
    "# Check skewness after transformation\n",
    "transformed_skewness = data.select(skewness(col('Auunal Emission'))).collect()[0][0]\n",
    "print(\"Skewness after transformation:\", transformed_skewness)\n",
    "\n",
    "# Convert the PySpark DataFrame to a Pandas DataFrame for plotting\n",
    "data_pd = data.toPandas()\n",
    "\n",
    "# Plotting the distribution after log transformation\n",
    "plt.figure(figsize=(12, 6))\n",
    "data_pd['Auunal Emission'].hist(bins=30)\n",
    "plt.title('Log Transformed Distribution of Emissions')\n",
    "plt.xlabel('Log of Auunal Emission')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the transformed dataset\n",
    "transformed_file_path = 'Transformed_Five_Year_Aggregated_Data.csv'\n",
    "data.coalesce(1).write.csv(transformed_file_path, header=True, mode='overwrite')\n",
    "\n",
    "print(\"Transformed data saved successfully.\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf56ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ARIMA_Modeling\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load time series data\n",
    "file_path = 'Transformed_Five_Year_Aggregated_Data.csv'\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display the columns of the dataset\n",
    "data.printSchema()\n",
    "\n",
    "# Ensure 'Five_Year_Block' is in the correct format\n",
    "data = data.withColumn('Five_Year_Block', col('Five_Year_Block').cast('int'))\n",
    "\n",
    "# Convert to Pandas DataFrame for ARIMA modeling\n",
    "data_pd = data.toPandas()\n",
    "\n",
    "# Assuming 'Five_Year_Block' is the index and 'Auunal Emission' is the time series column you want to model\n",
    "data_pd['Five_Year_Block'] = pd.to_datetime(data_pd['Five_Year_Block'], format='%Y')\n",
    "data_pd.set_index('Five_Year_Block', inplace=True)\n",
    "\n",
    "# Example parameter settings after exploratory analysis\n",
    "model = ARIMA(data_pd['Auunal Emission'], order=(1, 1, 1))  # These parameters should be refined based on your data\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Print summary of the model to review performance and diagnostics\n",
    "print(fitted_model.summary())\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeans_Clustering\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'Transformed_Five_Year_Aggregated_Data.csv'\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the columns of the dataset to verify\n",
    "data.printSchema()\n",
    "\n",
    "# Assemble features into a feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Auunal Emission', 'Cumulative_Anomaly'],\n",
    "    outputCol='features'\n",
    ")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Elbow method to determine the optimal number of clusters\n",
    "wcss = []\n",
    "for i in range(2, 11):  \n",
    "    kmeans = KMeans().setK(i).setSeed(1)\n",
    "    model = kmeans.fit(data.select('features'))\n",
    "    wcss.append(model.summary.trainingCost)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(range(2, 11), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')  # Within-cluster sum of squares\n",
    "plt.show()\n",
    "\n",
    "# Fitting K-Means to the dataset\n",
    "kmeans = KMeans().setK(3).setSeed(1)  # Adjust the number of clusters based on the elbow curve\n",
    "model = kmeans.fit(data.select('features'))\n",
    "data = model.transform(data)\n",
    "\n",
    "# Check the resulting clusters\n",
    "data.select('Auunal Emission', 'Cumulative_Anomaly', 'prediction').show(5)\n",
    "\n",
    "# Convert to Pandas DataFrame if needed for further analysis or visualization\n",
    "data_pd = data.select('Auunal Emission', 'Cumulative_Anomaly', 'prediction').toPandas()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a01ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ARIMA_Modeling\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Transformed_Five_Year_Aggregated_Data.csv'\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Ensure 'Five_Year_Block' is in the correct format\n",
    "data = data.withColumn('Five_Year_Block', col('Five_Year_Block').cast('int'))\n",
    "\n",
    "# Convert to Pandas DataFrame for ARIMA modeling\n",
    "data_pd = data.toPandas()\n",
    "\n",
    "# Assuming 'Five_Year_Block' is the index and 'Auunal Emission' is the time series column you want to model\n",
    "data_pd['Five_Year_Block'] = pd.to_datetime(data_pd['Five_Year_Block'], format='%Y')\n",
    "data_pd.set_index('Five_Year_Block', inplace=True)\n",
    "data_pd.index = pd.DatetimeIndex(data_pd.index).to_period('Y')  # Ensure yearly frequency\n",
    "\n",
    "# Fit the ARIMA model\n",
    "model = ARIMA(data_pd['Auunal Emission'], order=(1, 1, 1))\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Forecast future points and handle index correctly\n",
    "forecast = fitted_model.forecast(steps=5)\n",
    "print(\"Forecasted Emissions:\", forecast)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "8\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ARIMA_Modeling\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Transformed_Five_Year_Aggregated_Data.csv'\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Ensure 'Five_Year_Block' is in the correct format\n",
    "data = data.withColumn('Five_Year_Block', col('Five_Year_Block').cast('int'))\n",
    "\n",
    "# Convert to Pandas DataFrame for ARIMA modeling\n",
    "data_pd = data.toPandas()\n",
    "\n",
    "# Assuming 'Five_Year_Block' is the index and 'Auunal Emission' is the time series column you want to model\n",
    "data_pd['Five_Year_Block'] = pd.to_datetime(data_pd['Five_Year_Block'], format='%Y')\n",
    "data_pd.set_index('Five_Year_Block', inplace=True)\n",
    "data_pd = data_pd.sort_index()\n",
    "\n",
    "# Fit the ARIMA model\n",
    "model = ARIMA(data_pd['Auunal Emission'], order=(1, 1, 1))\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Forecast future points\n",
    "forecast = fitted_model.forecast(steps=5)\n",
    "forecast_index = pd.date_range(start=data_pd.index.max(), periods=len(forecast) + 1, freq='A')[1:]\n",
    "\n",
    "# Plot historical data and forecasts\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(data_pd.index, data_pd['Auunal Emission'], label='Historical Emissions')\n",
    "plt.plot(forecast_index, forecast, label='Forecasted Emissions', color='red')\n",
    "plt.title('ARIMA Model Forecast')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Annual Emissions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "residuals = fitted_model.resid\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(residuals)\n",
    "plt.title('Residuals from ARIMA Model')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KMeans_Clustering\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "file_path = 'Transformed_Five_Year_Aggregated_Data.csv'\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Assemble features into a feature vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Auunal Emission', 'Cumulative_Anomaly'],\n",
    "    outputCol='features'\n",
    ")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Create and fit the K-Means model\n",
    "kmeans = KMeans().setK(3).setSeed(1)\n",
    "model = kmeans.fit(data.select('features'))\n",
    "data = model.transform(data)\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "data_pd = data.select('Auunal Emission', 'Cumulative_Anomaly', 'prediction').toPandas()\n",
    "\n",
    "# Plot the clustering scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data_pd['Auunal Emission'], data_pd['Cumulative_Anomaly'], c=data_pd['prediction'], cmap='viridis', marker='o')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.xlabel('Annual Emission')\n",
    "plt.ylabel('Cumulative Anomaly')\n",
    "plt.title('K-Means Clustering of Emission Data')\n",
    "plt.show()\n",
    "\n",
    "# Plot cluster centers\n",
    "centers = model.clusterCenters()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter([center[0] for center in centers], [center[1] for center in centers], c='red', s=200, alpha=0.75, marker='x')\n",
    "plt.xlabel('Annual Emission')\n",
    "plt.ylabel('Cumulative Anomaly')\n",
    "plt.title('Centers of K-Means Clusters')\n",
    "plt.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ecbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
